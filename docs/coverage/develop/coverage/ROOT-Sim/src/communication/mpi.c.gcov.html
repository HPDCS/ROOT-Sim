<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">

<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>LCOV - doc-coverage.info - ROOT-Sim/src/communication/mpi.c</title>
  <link rel="stylesheet" type="text/css" href="../../../gcov.css">
</head>

<body>

  <table width="100%" border=0 cellspacing=0 cellpadding=0>
    <tr><td class="title">LCOV - code coverage report</td></tr>
    <tr><td class="ruler"><img src="../../../glass.png" width=3 height=3 alt=""></td></tr>

    <tr>
      <td width="100%">
        <table cellpadding=1 border=0 width="100%">
          <tr>
            <td width="10%" class="headerItem">Current view:</td>
            <td width="35%" class="headerValue"><a href="../../../index.html">top level</a> - <a href="index.html">ROOT-Sim/src/communication</a> - mpi.c</td>
            <td width="5%"></td>
            <td width="15%"></td>
            <td width="10%" class="headerCovTableHead">Hit</td>
            <td width="10%" class="headerCovTableHead">Total</td>
            <td width="15%" class="headerCovTableHead">Coverage</td>
          </tr>
          <tr>
            <td class="headerItem">Test:</td>
            <td class="headerValue">doc-coverage.info</td>
            <td></td>
            <td class="headerItem">Lines:</td>
            <td class="headerCovTableEntry">28</td>
            <td class="headerCovTableEntry">28</td>
            <td class="headerCovTableEntryHi">100.0 %</td>
          </tr>
          <tr>
            <td class="headerItem">Date:</td>
            <td class="headerValue">2020-01-31 15:54:55</td>
            <td></td>
          </tr>
          <tr><td><img src="../../../glass.png" width=3 height=3 alt=""></td></tr>
        </table>
      </td>
    </tr>

    <tr><td class="ruler"><img src="../../../glass.png" width=3 height=3 alt=""></td></tr>
  </table>

  <table cellpadding=0 cellspacing=0 border=0>
    <tr>
      <td><br></td>
    </tr>
    <tr>
      <td>
<pre class="sourceHeading">          Line data    Source code</pre>
<pre class="source">
<span class="lineNum">       1 </span><span class="lineCov">          1 : /**</span>
<span class="lineNum">       2 </span>            : * @file communication/mpi.c
<span class="lineNum">       3 </span>            : *
<span class="lineNum">       4 </span>            : * @brief MPI Support Module
<span class="lineNum">       5 </span>            : *
<span class="lineNum">       6 </span>            : * This module implements all basic MPI facilities to let the distributed
<span class="lineNum">       7 </span>            : * execution of a simulation model take place consistently.
<span class="lineNum">       8 </span>            : *
<span class="lineNum">       9 </span>            : * Several facilities are thread-safe, others are not. Check carefully which
<span class="lineNum">      10 </span>            : * of these can be used by worker threads without coordination when relying
<span class="lineNum">      11 </span>            : * on this module.
<span class="lineNum">      12 </span>            : *
<span class="lineNum">      13 </span>            : * @copyright
<span class="lineNum">      14 </span>            : * Copyright (C) 2008-2019 HPDCS Group
<span class="lineNum">      15 </span>            : * https://hpdcs.github.io
<span class="lineNum">      16 </span>            : *
<span class="lineNum">      17 </span>            : * This file is part of ROOT-Sim (ROme OpTimistic Simulator).
<span class="lineNum">      18 </span>            : *
<span class="lineNum">      19 </span>            : * ROOT-Sim is free software; you can redistribute it and/or modify it under the
<span class="lineNum">      20 </span>            : * terms of the GNU General Public License as published by the Free Software
<span class="lineNum">      21 </span>            : * Foundation; only version 3 of the License applies.
<span class="lineNum">      22 </span>            : *
<span class="lineNum">      23 </span>            : * ROOT-Sim is distributed in the hope that it will be useful, but WITHOUT ANY
<span class="lineNum">      24 </span>            : * WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR
<span class="lineNum">      25 </span>            : * A PARTICULAR PURPOSE. See the GNU General Public License for more details.
<span class="lineNum">      26 </span>            : *
<span class="lineNum">      27 </span>            : * You should have received a copy of the GNU General Public License along with
<span class="lineNum">      28 </span>            : * ROOT-Sim; if not, write to the Free Software Foundation, Inc.,
<span class="lineNum">      29 </span>            : * 51 Franklin St, Fifth Floor, Boston, MA  02110-1301  USA
<span class="lineNum">      30 </span>            : *
<span class="lineNum">      31 </span>            : * @author Tommaso Tocci
<span class="lineNum">      32 </span>            : */
<span class="lineNum">      33 </span>            : 
<span class="lineNum">      34 </span>            : #ifdef HAVE_MPI
<span class="lineNum">      35 </span>            : 
<span class="lineNum">      36 </span>            : #include &lt;stdbool.h&gt;
<span class="lineNum">      37 </span>            : 
<span class="lineNum">      38 </span>            : #include &lt;communication/mpi.h&gt;
<span class="lineNum">      39 </span>            : #include &lt;communication/wnd.h&gt;
<span class="lineNum">      40 </span>            : #include &lt;communication/gvt.h&gt;
<span class="lineNum">      41 </span>            : #include &lt;communication/communication.h&gt;
<span class="lineNum">      42 </span>            : #include &lt;queues/queues.h&gt;
<span class="lineNum">      43 </span>            : #include &lt;core/core.h&gt;
<span class="lineNum">      44 </span>            : #include &lt;arch/atomic.h&gt;
<span class="lineNum">      45 </span>            : #include &lt;statistics/statistics.h&gt;
<span class="lineNum">      46 </span>            : 
<span class="lineNum">      47 </span>            : /// Flag telling whether the MPI runtime supports multithreading
<span class="lineNum">      48 </span><span class="lineCov">          1 : bool mpi_support_multithread;</span>
<span class="lineNum">      49 </span>            : 
<span class="lineNum">      50 </span>            : /**
<span class="lineNum">      51 </span>            :  * This global lock is used by the lock/unlock_mpi macro to
<span class="lineNum">      52 </span>            :  * control access to MPI interface. If proper MPI threading support
<span class="lineNum">      53 </span>            :  * is available from the runtime, then it is not used.
<span class="lineNum">      54 </span>            :  */
<span class="lineNum">      55 </span><span class="lineCov">          1 : spinlock_t mpi_lock;</span>
<span class="lineNum">      56 </span>            : 
<span class="lineNum">      57 </span>            : /// A guard to ensure isolation in the the message receiving routine
<span class="lineNum">      58 </span><span class="lineCov">          1 : static spinlock_t msgs_lock;</span>
<span class="lineNum">      59 </span>            : 
<span class="lineNum">      60 </span>            : /**
<span class="lineNum">      61 </span>            :  * This counter tells how many simulation kernel instances have already
<span class="lineNum">      62 </span>            :  * reached the termination condition. This is updated via collect_termination().
<span class="lineNum">      63 </span>            :  */
<span class="lineNum">      64 </span><span class="lineCov">          1 : static unsigned int terminated = 0;</span>
<span class="lineNum">      65 </span>            : 
<span class="lineNum">      66 </span>            : /// MPI Requests to handle termination detection collection asynchronously
<span class="lineNum">      67 </span><span class="lineCov">          1 : static MPI_Request *termination_reqs;</span>
<span class="lineNum">      68 </span>            : 
<span class="lineNum">      69 </span>            : /// A guard to ensure isolation in collect_termination()
<span class="lineNum">      70 </span><span class="lineCov">          1 : static spinlock_t msgs_fini;</span>
<span class="lineNum">      71 </span>            : 
<span class="lineNum">      72 </span>            : /// MPI Operation to reduce statics
<span class="lineNum">      73 </span><span class="lineCov">          1 : static MPI_Op reduce_stats_op;</span>
<span class="lineNum">      74 </span>            : 
<span class="lineNum">      75 </span>            : /// MPI Datatype to describe the content of a struct @ref stat_t
<span class="lineNum">      76 </span><span class="lineCov">          1 : static MPI_Datatype stats_mpi_t;</span>
<span class="lineNum">      77 </span>            : 
<span class="lineNum">      78 </span>            : /**
<span class="lineNum">      79 </span>            :  * @brief MPI Communicator for event/control messages.
<span class="lineNum">      80 </span>            :  *
<span class="lineNum">      81 </span>            :  * To enable zero-copy message passing, we must know what LP is the destination
<span class="lineNum">      82 </span>            :  * of an event, @e before extracting that event from the MPI layer. This
<span class="lineNum">      83 </span>            :  * is necessary to determine from what slab/buddy the memory to keep
<span class="lineNum">      84 </span>            :  * the event must be taken. Yet, this is impossible because the MPI layer
<span class="lineNum">      85 </span>            :  * does not allow to do so.
<span class="lineNum">      86 </span>            :  *
<span class="lineNum">      87 </span>            :  * To actually be able to do that, the trick is to create a separate
<span class="lineNum">      88 </span>            :  * MPI Communicator which is used @e only to exchance events across LPs
<span class="lineNum">      89 </span>            :  * (control messages also fall in this category). Then, since we can
<span class="lineNum">      90 </span>            :  * extract events from this communicator, we can match against both
<span class="lineNum">      91 </span>            :  * MPI_ANY_SOURCE (to receive events from any simulation kernel instance)
<span class="lineNum">      92 </span>            :  * and MPI_ANY_TAG (to match independently of the tag).
<span class="lineNum">      93 </span>            :  *
<span class="lineNum">      94 </span>            :  * We therefore use the tag to identify the GID of the LP.
<span class="lineNum">      95 </span>            :  *
<span class="lineNum">      96 </span>            :  * We can retrieve the information about the message sender and the size
<span class="lineNum">      97 </span>            :  * of the message which will be extracted by inspecting the MPI_Status
<span class="lineNum">      98 </span>            :  * variable after an MPI_Iprobe is completed.
<span class="lineNum">      99 </span>            :  */
<span class="lineNum">     100 </span><span class="lineCov">          1 : static MPI_Comm msg_comm;</span>
<span class="lineNum">     101 </span>            : 
<span class="lineNum">     102 </span>            : 
<span class="lineNum">     103 </span>            : /**
<span class="lineNum">     104 </span>            :  * @brief Check if there are pending messages
<span class="lineNum">     105 </span>            :  *
<span class="lineNum">     106 </span>            :  * This function tells whether there is a pending message in the underlying
<span class="lineNum">     107 </span>            :  * MPI library coming from any remote simulation kernel instance. If passing
<span class="lineNum">     108 </span>            :  * a tag different from MPI_ANY_TAG to this function, a specific tag can
<span class="lineNum">     109 </span>            :  * be extracted.
<span class="lineNum">     110 </span>            :  *
<span class="lineNum">     111 </span>            :  * Messages are only extracted from MPI_COMM_WORLD communicator.  This is
<span class="lineNum">     112 </span>            :  * therefore only useful in startup/shutdown operations (this is used
<span class="lineNum">     113 </span>            :  * indeed to initiate GVT and conclude the distributed simulation shutdown).
<span class="lineNum">     114 </span>            :  *
<span class="lineNum">     115 </span>            :  * @note This function is thread-safe.
<span class="lineNum">     116 </span>            :  *
<span class="lineNum">     117 </span>            :  * @param tag The tag of the messages to check for availability.
<span class="lineNum">     118 </span>            :  *
<span class="lineNum">     119 </span>            :  * @return @c true if a pending message tagged with @p tag is found,
<span class="lineNum">     120 </span>            :  *         @c false otherwise.
<span class="lineNum">     121 </span>            :  */
<span class="lineNum">     122 </span><span class="lineCov">          1 : bool pending_msgs(int tag)</span>
<span class="lineNum">     123 </span>            : {
<span class="lineNum">     124 </span>            :         int flag = 0;
<span class="lineNum">     125 </span>            :         lock_mpi();
<span class="lineNum">     126 </span>            :         MPI_Iprobe(MPI_ANY_SOURCE, tag, MPI_COMM_WORLD, &amp;flag, MPI_STATUS_IGNORE);
<span class="lineNum">     127 </span>            :         unlock_mpi();
<span class="lineNum">     128 </span>            :         return (bool)flag;
<span class="lineNum">     129 </span>            : }
<span class="lineNum">     130 </span>            : 
<span class="lineNum">     131 </span>            : /**
<span class="lineNum">     132 </span>            :  * @brief check if an MPI request has been completed
<span class="lineNum">     133 </span>            :  *
<span class="lineNum">     134 </span>            :  * This function checks whether the operation associated with the specified
<span class="lineNum">     135 </span>            :  * MPI Request has been completed or not.
<span class="lineNum">     136 </span>            :  *
<span class="lineNum">     137 </span>            :  * @note This function is thread-safe.
<span class="lineNum">     138 </span>            :  *
<span class="lineNum">     139 </span>            :  * @param req A pointer to the MPI_Request to check for completion
<span class="lineNum">     140 </span>            :  *
<span class="lineNum">     141 </span>            :  * @return @c true if the operation associated with @p req is complete,
<span class="lineNum">     142 </span>            :  *         @c false otherwise.
<span class="lineNum">     143 </span>            :  */
<span class="lineNum">     144 </span><span class="lineCov">          1 : bool is_request_completed(MPI_Request *req)</span>
<span class="lineNum">     145 </span>            : {
<span class="lineNum">     146 </span>            :         int flag = 0;
<span class="lineNum">     147 </span>            :         lock_mpi();
<span class="lineNum">     148 </span>            :         MPI_Test(req, &amp;flag, MPI_STATUS_IGNORE);
<span class="lineNum">     149 </span>            :         unlock_mpi();
<span class="lineNum">     150 </span>            :         return (bool)flag;
<span class="lineNum">     151 </span>            : }
<span class="lineNum">     152 </span>            : 
<span class="lineNum">     153 </span>            : /**
<span class="lineNum">     154 </span>            :  * @brief Send a message to a remote LP
<span class="lineNum">     155 </span>            :  *
<span class="lineNum">     156 </span>            :  * This function takes in charge an event to be delivered to a remote LP.
<span class="lineNum">     157 </span>            :  * The sending operation is non-blocking: to this end, the message is
<span class="lineNum">     158 </span>            :  * registered into the outgoing queue of the destination kernel, in order
<span class="lineNum">     159 </span>            :  * to allow MPI to keep track of the sending operation.
<span class="lineNum">     160 </span>            :  *
<span class="lineNum">     161 </span>            :  * Also, the message being sent is registered at the sender thread, to
<span class="lineNum">     162 </span>            :  * keep track of the white/red message information which is necessary
<span class="lineNum">     163 </span>            :  * to correctly reduce the GVT value.
<span class="lineNum">     164 </span>            :  *
<span class="lineNum">     165 </span>            :  * @note This function is thread-safe.
<span class="lineNum">     166 </span>            :  *
<span class="lineNum">     167 </span>            :  * @param msg A pointer to the @ref msg_t keeping the message to be sent remotely
<span class="lineNum">     168 </span>            :  */
<span class="lineNum">     169 </span><span class="lineCov">          1 : void send_remote_msg(msg_t *msg)</span>
<span class="lineNum">     170 </span>            : {
<span class="lineNum">     171 </span>            :         outgoing_msg *out_msg = allocate_outgoing_msg();
<span class="lineNum">     172 </span>            :         out_msg-&gt;msg = msg;
<span class="lineNum">     173 </span>            :         out_msg-&gt;msg-&gt;colour = threads_phase_colour[local_tid];
<span class="lineNum">     174 </span>            :         unsigned int dest = find_kernel_by_gid(msg-&gt;receiver);
<span class="lineNum">     175 </span>            : 
<span class="lineNum">     176 </span>            :         validate_msg(msg);
<span class="lineNum">     177 </span>            : 
<span class="lineNum">     178 </span>            :         register_outgoing_msg(out_msg-&gt;msg);
<span class="lineNum">     179 </span>            : 
<span class="lineNum">     180 </span>            :         lock_mpi();
<span class="lineNum">     181 </span>            :         MPI_Isend(((char *)out_msg-&gt;msg) + MSG_PADDING, MSG_META_SIZE + msg-&gt;size, MPI_BYTE, dest, msg-&gt;receiver.to_int, msg_comm, &amp;out_msg-&gt;req);
<span class="lineNum">     182 </span>            :         unlock_mpi();
<span class="lineNum">     183 </span>            : 
<span class="lineNum">     184 </span>            :         // Keep the message in the outgoing queue until it will be delivered
<span class="lineNum">     185 </span>            :         store_outgoing_msg(out_msg, dest);
<span class="lineNum">     186 </span>            : }
<span class="lineNum">     187 </span>            : 
<span class="lineNum">     188 </span>            : /**
<span class="lineNum">     189 </span>            :  * @brief Receive remote messages
<span class="lineNum">     190 </span>            :  *
<span class="lineNum">     191 </span>            :  * This function extracts from MPI events destined to locally-hosted
<span class="lineNum">     192 </span>            :  * LPs. Only messages to LP can be extracted here, because the probing
<span class="lineNum">     193 </span>            :  * is done towards the @ref msg_comm communicator.
<span class="lineNum">     194 </span>            :  *
<span class="lineNum">     195 </span>            :  * A message which is extracted here is placed (out of order) in the
<span class="lineNum">     196 </span>            :  * bottom half of the destination LP, for later insertion (in order) in
<span class="lineNum">     197 </span>            :  * the input queue.
<span class="lineNum">     198 </span>            :  *
<span class="lineNum">     199 </span>            :  * This function will try to extract as many messages as possible from
<span class="lineNum">     200 </span>            :  * the underlying MPI library. In particular, once this function is
<span class="lineNum">     201 </span>            :  * called, it will return only after that @e no @e message can be found
<span class="lineNum">     202 </span>            :  * in the MPI library, destined to this simulation kernel instance.
<span class="lineNum">     203 </span>            :  *
<span class="lineNum">     204 </span>            :  * Currently, this function is called once per main loop iteration. Doing
<span class="lineNum">     205 </span>            :  * more calls might significantly imbalance the workload of some worker
<span class="lineNum">     206 </span>            :  * thread.
<span class="lineNum">     207 </span>            :  *
<span class="lineNum">     208 </span>            :  * @note This function is thread-safe.
<span class="lineNum">     209 </span>            :  */
<span class="lineNum">     210 </span><span class="lineCov">          1 : void receive_remote_msgs(void)</span>
<span class="lineNum">     211 </span>            : {
<span class="lineNum">     212 </span>            :         int size;
<span class="lineNum">     213 </span>            :         msg_t *msg;
<span class="lineNum">     214 </span>            :         MPI_Status status;
<span class="lineNum">     215 </span>            :         MPI_Message mpi_msg;
<span class="lineNum">     216 </span>            :         int pending;
<span class="lineNum">     217 </span>            :         struct lp_struct *lp;
<span class="lineNum">     218 </span>            :         GID_t gid;
<span class="lineNum">     219 </span>            : 
<span class="lineNum">     220 </span>            :         // TODO: given the latest changes in the platform, this *might*
<span class="lineNum">     221 </span>            :         // be removed.
<span class="lineNum">     222 </span>            :         if (!spin_trylock(&amp;msgs_lock))
<span class="lineNum">     223 </span>            :                 return;
<span class="lineNum">     224 </span>            : 
<span class="lineNum">     225 </span>            :         while (true) {
<span class="lineNum">     226 </span>            :                 lock_mpi();
<span class="lineNum">     227 </span>            :                 MPI_Improbe(MPI_ANY_SOURCE, MPI_ANY_TAG, msg_comm, &amp;pending, &amp;mpi_msg, &amp;status);
<span class="lineNum">     228 </span>            :                 unlock_mpi();
<span class="lineNum">     229 </span>            : 
<span class="lineNum">     230 </span>            :                 if (!pending)
<span class="lineNum">     231 </span>            :                         goto out;
<span class="lineNum">     232 </span>            : 
<span class="lineNum">     233 </span>            :                 MPI_Get_count(&amp;status, MPI_BYTE, &amp;size);
<span class="lineNum">     234 </span>            : 
<span class="lineNum">     235 </span>            :                 if (likely(MSG_PADDING + size &lt;= SLAB_MSG_SIZE)) {
<span class="lineNum">     236 </span>            :                         set_gid(gid, status.MPI_TAG);
<span class="lineNum">     237 </span>            :                         lp = find_lp_by_gid(gid);
<span class="lineNum">     238 </span>            :                         msg = get_msg_from_slab(lp);
<span class="lineNum">     239 </span>            :                 } else {
<span class="lineNum">     240 </span>            :                         msg = rsalloc(MSG_PADDING + size);
<span class="lineNum">     241 </span>            :                         bzero(msg, MSG_PADDING);
<span class="lineNum">     242 </span>            :                 }
<span class="lineNum">     243 </span>            : 
<span class="lineNum">     244 </span>            :                 // Receive the message. Use MPI_Mrecv to be sure that the very same message
<span class="lineNum">     245 </span>            :                 // which was matched by the previous MPI_Improbe is extracted.
<span class="lineNum">     246 </span>            :                 lock_mpi();
<span class="lineNum">     247 </span>            :                 MPI_Mrecv(((char *)msg) + MSG_PADDING, size, MPI_BYTE, &amp;mpi_msg, MPI_STATUS_IGNORE);
<span class="lineNum">     248 </span>            :                 unlock_mpi();
<span class="lineNum">     249 </span>            : 
<span class="lineNum">     250 </span>            :                 validate_msg(msg);
<span class="lineNum">     251 </span>            :                 insert_bottom_half(msg);
<span class="lineNum">     252 </span>            :         }
<span class="lineNum">     253 </span>            :     out:
<span class="lineNum">     254 </span>            :         spin_unlock(&amp;msgs_lock);
<span class="lineNum">     255 </span>            : }
<span class="lineNum">     256 </span>            : 
<span class="lineNum">     257 </span>            : 
<span class="lineNum">     258 </span>            : 
<span class="lineNum">     259 </span>            : /**
<span class="lineNum">     260 </span>            :  * @brief Check if all kernels have reached the termination condition
<span class="lineNum">     261 </span>            :  *
<span class="lineNum">     262 </span>            :  * This function checks whether all threads have been informed of the
<span class="lineNum">     263 </span>            :  * fact that the simulation should be halted, and they have taken
<span class="lineNum">     264 </span>            :  * proper actions to terminate. Once this function confirms this condition,
<span class="lineNum">     265 </span>            :  * the process can safely exit.
<span class="lineNum">     266 </span>            :  *
<span class="lineNum">     267 </span>            :  * @warning This function can be called only @b after a call to
<span class="lineNum">     268 </span>            :  *          broadcast_termination()
<span class="lineNum">     269 </span>            :  *
<span class="lineNum">     270 </span>            :  * @return @c true if all the kernel have reached the termination condition
<span class="lineNum">     271 </span>            :  */
<span class="lineNum">     272 </span><span class="lineCov">          1 : bool all_kernels_terminated(void)</span>
<span class="lineNum">     273 </span>            : {
<span class="lineNum">     274 </span>            :         return (terminated == n_ker);
<span class="lineNum">     275 </span>            : }
<span class="lineNum">     276 </span>            : 
<span class="lineNum">     277 </span>            : 
<span class="lineNum">     278 </span>            : 
<span class="lineNum">     279 </span>            : /**
<span class="lineNum">     280 </span>            :  * @brief Check if other kernels have reached the termination condition
<span class="lineNum">     281 </span>            :  *
<span class="lineNum">     282 </span>            :  * This function accumulates termination acknoledgements from remote
<span class="lineNum">     283 </span>            :  * kernels, and updates the @ref terminated counter.
<span class="lineNum">     284 </span>            :  *
<span class="lineNum">     285 </span>            :  * @note This function can be called at any point of the simulation,
<span class="lineNum">     286 </span>            :  *       but it will be effective only after that broadcast_termination()
<span class="lineNum">     287 </span>            :  *       has been called locally.
<span class="lineNum">     288 </span>            :  *
<span class="lineNum">     289 </span>            :  * @note This function is thread-safe
<span class="lineNum">     290 </span>            :  */
<span class="lineNum">     291 </span><span class="lineCov">          1 : void collect_termination(void)</span>
<span class="lineNum">     292 </span>            : {
<span class="lineNum">     293 </span>            :         int res;
<span class="lineNum">     294 </span>            :         unsigned int tdata;
<span class="lineNum">     295 </span>            : 
<span class="lineNum">     296 </span>            :         if (terminated == 0 || !spin_trylock(&amp;msgs_fini))
<span class="lineNum">     297 </span>            :                 return;
<span class="lineNum">     298 </span>            : 
<span class="lineNum">     299 </span>            :         while (pending_msgs(MSG_FINI)) {
<span class="lineNum">     300 </span>            :                 lock_mpi();
<span class="lineNum">     301 </span>            :                 res =
<span class="lineNum">     302 </span>            :                     MPI_Recv(&amp;tdata, 1, MPI_UNSIGNED, MPI_ANY_SOURCE, MSG_FINI, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
<span class="lineNum">     303 </span>            :                 unlock_mpi();
<span class="lineNum">     304 </span>            :                 if (unlikely(res != 0)) {
<span class="lineNum">     305 </span>            :                         rootsim_error(true, &quot;MPI_Recv did not complete correctly&quot;);
<span class="lineNum">     306 </span>            :                         return;
<span class="lineNum">     307 </span>            :                 }
<span class="lineNum">     308 </span>            :                 terminated++;
<span class="lineNum">     309 </span>            :         }
<span class="lineNum">     310 </span>            :         spin_unlock(&amp;msgs_fini);
<span class="lineNum">     311 </span>            : }
<span class="lineNum">     312 </span>            : 
<span class="lineNum">     313 </span>            : 
<span class="lineNum">     314 </span>            : /**
<span class="lineNum">     315 </span>            :  * @brief Notify all the kernels about local termination
<span class="lineNum">     316 </span>            :  *
<span class="lineNum">     317 </span>            :  * This function is used to inform all other simulation kernel instances
<span class="lineNum">     318 </span>            :  * that this kernel is ready to terminate the simulation.
<span class="lineNum">     319 </span>            :  *
<span class="lineNum">     320 </span>            :  * @warning This function is not thread safe and should be used only
<span class="lineNum">     321 </span>            :  *          by one thread at a time
<span class="lineNum">     322 </span>            :  *
<span class="lineNum">     323 </span>            :  * @note This function can be used concurrently with other MPI functions
<span class="lineNum">     324 </span>            :  *       (hence its thread unsafety)
<span class="lineNum">     325 </span>            :  *
<span class="lineNum">     326 </span>            :  * @note This function can be called multiple times, but the actual
<span class="lineNum">     327 </span>            :  *       broadcast operation will be executed only on the first call.
<span class="lineNum">     328 </span>            :  */
<span class="lineNum">     329 </span><span class="lineCov">          1 : void broadcast_termination(void)</span>
<span class="lineNum">     330 </span>            : {
<span class="lineNum">     331 </span>            :         unsigned int i;
<span class="lineNum">     332 </span>            :         lock_mpi();
<span class="lineNum">     333 </span>            :         for (i = 0; i &lt; n_ker; i++) {
<span class="lineNum">     334 </span>            :                 if (i == kid)
<span class="lineNum">     335 </span>            :                         continue;
<span class="lineNum">     336 </span>            :                 MPI_Isend(&amp;i, 1, MPI_UNSIGNED, i, MSG_FINI, MPI_COMM_WORLD, &amp;termination_reqs[i]);
<span class="lineNum">     337 </span>            :         }
<span class="lineNum">     338 </span>            :         terminated++;
<span class="lineNum">     339 </span>            :         unlock_mpi();
<span class="lineNum">     340 </span>            : }
<span class="lineNum">     341 </span>            : 
<span class="lineNum">     342 </span>            : 
<span class="lineNum">     343 </span>            : /**
<span class="lineNum">     344 </span>            :  * @brief Reduce operation for statistics.
<span class="lineNum">     345 </span>            :  *
<span class="lineNum">     346 </span>            :  * This function implements a custom MPI Operation used to reduce globally
<span class="lineNum">     347 </span>            :  * local statistics upon simulation shutdown. This function is bound to
<span class="lineNum">     348 </span>            :  * @ref reduce_stats_op in stats_reduction_init().
<span class="lineNum">     349 </span>            :  */
<span class="lineNum">     350 </span><span class="lineCov">          1 : static void reduce_stat_vector(struct stat_t *in, struct stat_t *inout, int *len, MPI_Datatype *dptr)</span>
<span class="lineNum">     351 </span>            : {
<span class="lineNum">     352 </span>            :         (void)dptr;
<span class="lineNum">     353 </span>            :         int i = 0;
<span class="lineNum">     354 </span>            : 
<span class="lineNum">     355 </span>            :         for (i = 0; i &lt; *len; ++i) {
<span class="lineNum">     356 </span>            :                 inout[i].vec += in[i].vec;
<span class="lineNum">     357 </span>            :                 inout[i].gvt_round_time += in[i].gvt_round_time;
<span class="lineNum">     358 </span>            :                 inout[i].gvt_round_time_min = fmin(inout[i].gvt_round_time_min, in[i].gvt_round_time_min);
<span class="lineNum">     359 </span>            :                 inout[i].gvt_round_time_max = fmax(inout[i].gvt_round_time_max, in[i].gvt_round_time_max);
<span class="lineNum">     360 </span>            :                 inout[i].max_resident_set += in[i].max_resident_set;
<span class="lineNum">     361 </span>            :         }
<span class="lineNum">     362 </span>            : }
<span class="lineNum">     363 </span>            : 
<span class="lineNum">     364 </span>            : 
<span class="lineNum">     365 </span>            : 
<span class="lineNum">     366 </span>            : /// The size in bytes of the statistics custom MPI Datatype. It assumes that @ref stat_t contains only double floating point members
<span class="lineNum">     367 </span><span class="lineCov">          1 : #define MPI_TYPE_STAT_LEN (sizeof(struct stat_t)/sizeof(double))</span>
<span class="lineNum">     368 </span>            : 
<span class="lineNum">     369 </span>            : /**
<span class="lineNum">     370 </span>            :  * @brief Initialize MPI Datatype and Operation for statistics reduction
<span class="lineNum">     371 </span>            :  *
<span class="lineNum">     372 </span>            :  * To reduce statistics, we rely on a custom MPI Operation. This operation
<span class="lineNum">     373 </span>            :  * requires a pre-built MPI Datatype to properly handle the structures which
<span class="lineNum">     374 </span>            :  * we use to represent the local information.
<span class="lineNum">     375 </span>            :  *
<span class="lineNum">     376 </span>            :  * This function is called when initializing inter-kernel communication,
<span class="lineNum">     377 </span>            :  * and its purpose is exactly that of setting up a custom MPI datatype in
<span class="lineNum">     378 </span>            :  * @ref stats_mpi_t.
<span class="lineNum">     379 </span>            :  *
<span class="lineNum">     380 </span>            :  * Additionally, this function defines the custom operation implemented in
<span class="lineNum">     381 </span>            :  * reduce_stat_vector() which is bound to the MPI Operation @ref reduce_stats_op.
<span class="lineNum">     382 </span>            :  */
<span class="lineNum">     383 </span><span class="lineCov">          1 : static void stats_reduction_init(void)</span>
<span class="lineNum">     384 </span>            : {
<span class="lineNum">     385 </span>            :         // This is a compilation time fail-safe
<span class="lineNum">     386 </span>            :         static_assert(offsetof(struct stat_t, gvt_round_time_max) == (sizeof(double) * 19), &quot;The packing assumptions on struct stat_t are wrong or its definition has been modified&quot;);
<span class="lineNum">     387 </span>            : 
<span class="lineNum">     388 </span>            :         unsigned i;
<span class="lineNum">     389 </span>            : 
<span class="lineNum">     390 </span>            :         // Boilerplate to create a new MPI data type
<span class="lineNum">     391 </span>            :         MPI_Datatype type[MPI_TYPE_STAT_LEN];
<span class="lineNum">     392 </span>            :         MPI_Aint disp[MPI_TYPE_STAT_LEN];
<span class="lineNum">     393 </span>            :         int block_lengths[MPI_TYPE_STAT_LEN];
<span class="lineNum">     394 </span>            : 
<span class="lineNum">     395 </span>            :         // Initialize those arrays (we asssume that struct stat_t is packed tightly)
<span class="lineNum">     396 </span>            :         for (i = 0; i &lt; MPI_TYPE_STAT_LEN; ++i) {
<span class="lineNum">     397 </span>            :                 type[i] = MPI_DOUBLE;
<span class="lineNum">     398 </span>            :                 disp[i] = i * sizeof(double);
<span class="lineNum">     399 </span>            :                 block_lengths[i] = 1;
<span class="lineNum">     400 </span>            :         }
<span class="lineNum">     401 </span>            : 
<span class="lineNum">     402 </span>            :         // Create the custom type and commit the changes
<span class="lineNum">     403 </span>            :         MPI_Type_create_struct(MPI_TYPE_STAT_LEN, block_lengths, disp, type, &amp;stats_mpi_t);
<span class="lineNum">     404 </span>            :         MPI_Type_commit(&amp;stats_mpi_t);
<span class="lineNum">     405 </span>            : 
<span class="lineNum">     406 </span>            :         // Create the MPI Operation used to reduce stats
<span class="lineNum">     407 </span>            :         if (master_thread()) {
<span class="lineNum">     408 </span>            :                 MPI_Op_create((MPI_User_function *)reduce_stat_vector, true, &amp;reduce_stats_op);
<span class="lineNum">     409 </span>            :         }
<span class="lineNum">     410 </span>            : }
<span class="lineNum">     411 </span>            : 
<span class="lineNum">     412 </span>            : #undef MPI_TYPE_STAT_LEN
<span class="lineNum">     413 </span>            : 
<span class="lineNum">     414 </span>            : 
<span class="lineNum">     415 </span>            : /**
<span class="lineNum">     416 </span>            :  * @brief Invoke statistics reduction.
<span class="lineNum">     417 </span>            :  *
<span class="lineNum">     418 </span>            :  * This function is a simple wrapper of an MPI_Reduce operation, which
<span class="lineNum">     419 </span>            :  * uses the custom reduce operation implemented in reduce_stat_vector()
<span class="lineNum">     420 </span>            :  * to gather reduced statistics in the master kernel (rank 0).
<span class="lineNum">     421 </span>            :  *
<span class="lineNum">     422 </span>            :  * @param global A pointer to a struct @ref stat_t where reduced statistics
<span class="lineNum">     423 </span>            :  *               will be stored. The reduction only takes place at rank 0,
<span class="lineNum">     424 </span>            :  *               therefore other simulation kernel instances will never
<span class="lineNum">     425 </span>            :  *               read actual meaningful information in that structure.
<span class="lineNum">     426 </span>            :  * @param local A pointer to a local struct @ref stat_t which is used
<span class="lineNum">     427 </span>            :  *               as the source of information for the distributed reduction
<span class="lineNum">     428 </span>            :  *               operation.
<span class="lineNum">     429 </span>            :  */
<span class="lineNum">     430 </span><span class="lineCov">          1 : void mpi_reduce_statistics(struct stat_t *global, struct stat_t *local)</span>
<span class="lineNum">     431 </span>            : {
<span class="lineNum">     432 </span>            :         MPI_Reduce(local, global, 1, stats_mpi_t, reduce_stats_op, 0, MPI_COMM_WORLD);
<span class="lineNum">     433 </span>            : }
<span class="lineNum">     434 </span>            : 
<span class="lineNum">     435 </span>            : 
<span class="lineNum">     436 </span>            : 
<span class="lineNum">     437 </span>            : /**
<span class="lineNum">     438 </span>            :  * @brief Setup the distributed termination subsystem
<span class="lineNum">     439 </span>            :  *
<span class="lineNum">     440 </span>            :  * To correctly terminate a distributed simulation, some care must be
<span class="lineNum">     441 </span>            :  * taken. In particular:
<span class="lineNum">     442 </span>            :  * * we must be use that no deadlock is generated, e.g. because some
<span class="lineNum">     443 </span>            :  *   simulation kernel is already waiting for some synchronization action
<span class="lineNum">     444 </span>            :  *   by other kernels
<span class="lineNum">     445 </span>            :  * * we must be sure that no MPI action is in place/still pending, when
<span class="lineNum">     446 </span>            :  *   MPI_Finalize() is called.
<span class="lineNum">     447 </span>            :  *
<span class="lineNum">     448 </span>            :  * To this end, a specific distributed termination protocol is put in place,
<span class="lineNum">     449 </span>            :  * which requires some data structures to be available.
<span class="lineNum">     450 </span>            :  *
<span class="lineNum">     451 </span>            :  * This function initializes the subsystem and the datastructures which
<span class="lineNum">     452 </span>            :  * ensure a clean a nice shutdown of distributed simulations.
<span class="lineNum">     453 </span>            :  */
<span class="lineNum">     454 </span><span class="lineCov">          1 : void dist_termination_init(void)</span>
<span class="lineNum">     455 </span>            : {
<span class="lineNum">     456 </span>            :         /* init for collective termination */
<span class="lineNum">     457 </span>            :         termination_reqs = rsalloc(n_ker * sizeof(MPI_Request));
<span class="lineNum">     458 </span>            :         unsigned int i;
<span class="lineNum">     459 </span>            :         for (i = 0; i &lt; n_ker; i++) {
<span class="lineNum">     460 </span>            :                 termination_reqs[i] = MPI_REQUEST_NULL;
<span class="lineNum">     461 </span>            :         }
<span class="lineNum">     462 </span>            :         spinlock_init(&amp;msgs_fini);
<span class="lineNum">     463 </span>            : }
<span class="lineNum">     464 </span>            : 
<span class="lineNum">     465 </span>            : 
<span class="lineNum">     466 </span>            : 
<span class="lineNum">     467 </span>            : /**
<span class="lineNum">     468 </span>            :  * @brief Cleanup routine of the distributed termination subsystem
<span class="lineNum">     469 </span>            :  *
<span class="lineNum">     470 </span>            :  * Once this function returns, it is sure that we can terminate safely
<span class="lineNum">     471 </span>            :  * the simulation.
<span class="lineNum">     472 </span>            :  */
<span class="lineNum">     473 </span><span class="lineCov">          1 : void dist_termination_finalize(void)</span>
<span class="lineNum">     474 </span>            : {
<span class="lineNum">     475 </span>            :         MPI_Waitall(n_ker, termination_reqs, MPI_STATUSES_IGNORE);
<span class="lineNum">     476 </span>            : }
<span class="lineNum">     477 </span>            : 
<span class="lineNum">     478 </span>            : /**
<span class="lineNum">     479 </span>            :  * @brief Syncronize all the kernels
<span class="lineNum">     480 </span>            :  *
<span class="lineNum">     481 </span>            :  * This function can be used as syncronization barrier between all the threads
<span class="lineNum">     482 </span>            :  * of all the kernels.
<span class="lineNum">     483 </span>            :  *
<span class="lineNum">     484 </span>            :  * The function will return only after all the threads on all the kernels
<span class="lineNum">     485 </span>            :  * have already entered this function.
<span class="lineNum">     486 </span>            :  *
<span class="lineNum">     487 </span>            :  * We create a new communicator here, to be sure that we synchronize
<span class="lineNum">     488 </span>            :  * exactly in this function and not somewhere else.
<span class="lineNum">     489 </span>            :  *
<span class="lineNum">     490 </span>            :  * @warning This function is extremely resource intensive, wastes a lot
<span class="lineNum">     491 </span>            :  *          of cpu cycles, and drops performance significantly. Avoid
<span class="lineNum">     492 </span>            :  *          using it as much as possible!
<span class="lineNum">     493 </span>            :  */
<span class="lineNum">     494 </span><span class="lineCov">          1 : void syncronize_all(void)</span>
<span class="lineNum">     495 </span>            : {
<span class="lineNum">     496 </span>            :         if (master_thread()) {
<span class="lineNum">     497 </span>            :                 MPI_Comm comm;
<span class="lineNum">     498 </span>            :                 MPI_Comm_dup(MPI_COMM_WORLD, &amp;comm);
<span class="lineNum">     499 </span>            :                 MPI_Barrier(comm);
<span class="lineNum">     500 </span>            :                 MPI_Comm_free(&amp;comm);
<span class="lineNum">     501 </span>            :         }
<span class="lineNum">     502 </span>            :         thread_barrier(&amp;all_thread_barrier);
<span class="lineNum">     503 </span>            : }
<span class="lineNum">     504 </span>            : 
<span class="lineNum">     505 </span>            : 
<span class="lineNum">     506 </span>            : /**
<span class="lineNum">     507 </span>            :  * @brief Initialize MPI subsystem
<span class="lineNum">     508 </span>            :  *
<span class="lineNum">     509 </span>            :  * This is mainly a wrapper of MPI_Init, which contains some boilerplate
<span class="lineNum">     510 </span>            :  * code to initialize datastructures.
<span class="lineNum">     511 </span>            :  *
<span class="lineNum">     512 </span>            :  * Most notably, here we determine if the library which we are using
<span class="lineNum">     513 </span>            :  * has suitable multithreading support, and we setup the MPI Communicator
<span class="lineNum">     514 </span>            :  * which will be used later on to exhange model-specific messages.
<span class="lineNum">     515 </span>            :  */
<span class="lineNum">     516 </span><span class="lineCov">          1 : void mpi_init(int *argc, char ***argv)</span>
<span class="lineNum">     517 </span>            : {
<span class="lineNum">     518 </span>            :         int mpi_thread_lvl_provided = 0;
<span class="lineNum">     519 </span>            :         MPI_Init_thread(argc, argv, MPI_THREAD_MULTIPLE, &amp;mpi_thread_lvl_provided);
<span class="lineNum">     520 </span>            : 
<span class="lineNum">     521 </span>            :         mpi_support_multithread = true;
<span class="lineNum">     522 </span>            :         if (mpi_thread_lvl_provided &lt; MPI_THREAD_MULTIPLE) {
<span class="lineNum">     523 </span>            :                 //MPI do not support thread safe api call
<span class="lineNum">     524 </span>            :                 if (mpi_thread_lvl_provided &lt; MPI_THREAD_SERIALIZED) {
<span class="lineNum">     525 </span>            :                         // MPI do not even support serialized threaded call we cannot continue
<span class="lineNum">     526 </span>            :                         rootsim_error(true, &quot;The MPI implementation does not support threads [current thread level support: %d]\n&quot;, mpi_thread_lvl_provided);
<span class="lineNum">     527 </span>            :                 }
<span class="lineNum">     528 </span>            :                 mpi_support_multithread = false;
<span class="lineNum">     529 </span>            :         }
<span class="lineNum">     530 </span>            : 
<span class="lineNum">     531 </span>            :         spinlock_init(&amp;mpi_lock);
<span class="lineNum">     532 </span>            : 
<span class="lineNum">     533 </span>            :         MPI_Comm_size(MPI_COMM_WORLD, (int *)&amp;n_ker);
<span class="lineNum">     534 </span>            :         MPI_Comm_rank(MPI_COMM_WORLD, (int *)&amp;kid);
<span class="lineNum">     535 </span>            : 
<span class="lineNum">     536 </span>            :         // Create a separate communicator which we use to send event messages
<span class="lineNum">     537 </span>            :         MPI_Comm_dup(MPI_COMM_WORLD, &amp;msg_comm);
<span class="lineNum">     538 </span>            : }
<span class="lineNum">     539 </span>            : 
<span class="lineNum">     540 </span>            : 
<span class="lineNum">     541 </span>            : /**
<span class="lineNum">     542 </span>            :  * @brief Initialize inter-kernel communication
<span class="lineNum">     543 </span>            :  *
<span class="lineNum">     544 </span>            :  * This function initializes inter-kernel communication, by initializing
<span class="lineNum">     545 </span>            :  * all the other communication subsystems.
<span class="lineNum">     546 </span>            :  */
<span class="lineNum">     547 </span><span class="lineCov">          1 : void inter_kernel_comm_init(void)</span>
<span class="lineNum">     548 </span>            : {
<span class="lineNum">     549 </span>            :         spinlock_init(&amp;msgs_lock);
<span class="lineNum">     550 </span>            : 
<span class="lineNum">     551 </span>            :         outgoing_window_init();
<span class="lineNum">     552 </span>            :         gvt_comm_init();
<span class="lineNum">     553 </span>            :         dist_termination_init();
<span class="lineNum">     554 </span>            :         stats_reduction_init();
<span class="lineNum">     555 </span>            : }
<span class="lineNum">     556 </span>            : 
<span class="lineNum">     557 </span>            : 
<span class="lineNum">     558 </span>            : /**
<span class="lineNum">     559 </span>            :  * @brief Finalize inter-kernel communication
<span class="lineNum">     560 </span>            :  *
<span class="lineNum">     561 </span>            :  * This function shutdown the subsystems associated with inter-kernel
<span class="lineNum">     562 </span>            :  * communication.
<span class="lineNum">     563 </span>            :  */
<span class="lineNum">     564 </span><span class="lineCov">          1 : void inter_kernel_comm_finalize(void)</span>
<span class="lineNum">     565 </span>            : {
<span class="lineNum">     566 </span>            :         dist_termination_finalize();
<span class="lineNum">     567 </span>            :         //outgoing_window_finalize();
<span class="lineNum">     568 </span>            :         gvt_comm_finalize();
<span class="lineNum">     569 </span>            : }
<span class="lineNum">     570 </span>            : 
<span class="lineNum">     571 </span>            : 
<span class="lineNum">     572 </span>            : /**
<span class="lineNum">     573 </span>            :  * @brief Finalize MPI
<span class="lineNum">     574 </span>            :  *
<span class="lineNum">     575 </span>            :  * This function shutdown the MPI subsystem
<span class="lineNum">     576 </span>            :  *
<span class="lineNum">     577 </span>            :  * @note Only the master thread on each simulation kernel is expected
<span class="lineNum">     578 </span>            :  *       to call this function
<span class="lineNum">     579 </span>            :  */
<span class="lineNum">     580 </span><span class="lineCov">          1 : void mpi_finalize(void)</span>
<span class="lineNum">     581 </span>            : {
<span class="lineNum">     582 </span>            :         if (master_thread()) {
<span class="lineNum">     583 </span>            :                 MPI_Barrier(MPI_COMM_WORLD);
<span class="lineNum">     584 </span>            :                 MPI_Comm_free(&amp;msg_comm);
<span class="lineNum">     585 </span>            :                 MPI_Finalize();
<span class="lineNum">     586 </span>            :         } else {
<span class="lineNum">     587 </span>            :                 rootsim_error(true, &quot;MPI finalize has been invoked by a non master thread: T%u\n&quot;, local_tid);
<span class="lineNum">     588 </span>            :         }
<span class="lineNum">     589 </span>            : }
<span class="lineNum">     590 </span>            : 
<span class="lineNum">     591 </span>            : #endif /* HAVE_MPI */
</pre>
      </td>
    </tr>
  </table>
  <br>

  <table width="100%" border=0 cellspacing=0 cellpadding=0>
    <tr><td class="ruler"><img src="../../../glass.png" width=3 height=3 alt=""></td></tr>
    <tr><td class="versionInfo">Generated by: <a href="http://ltp.sourceforge.net/coverage/lcov.php" target="_parent">LCOV version 1.12</a></td></tr>
  </table>
  <br>

</body>
</html>
